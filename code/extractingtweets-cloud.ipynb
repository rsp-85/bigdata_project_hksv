{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "# import re \n",
    "import string\n",
    "# import preprocessor as p\n",
    "# import swifter\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"even-hull-328204-0e032e418c72.json\"\n",
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAAxFUwEAAAAA9U4m4uvkk6%2FjGPrTFnZRykbnZfs%3DcsYq8RvBCSrDBAhxtRg4p6tiRRebcbQ0mEyqbXX9vxXOroWVcf',\n",
    "                      wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = 'ENJ3fXY9A2AncNwIpTwzAM4Mx'\n",
    "CONSUMER_SECRET = 'haYK0wRsXOiP1JU8QF2QcDtQtWroltcTcPxKYjJ95PP1vV5u5b'\n",
    "OAUTH_TOKEN= '408602898-HYpc9JUNnjjTkrkEZ8MhAdSzRAjQvD3XJ2DLJJMK'\n",
    "OAUTH_TOKEN_SECRET = 'J7IVJuDIrGPQe08pBieeUpn3mqvBtHUFU4iNkS3QpBQb4'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# tweet = api.get_status('1213330173736738817')\n",
    "# print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcp(out_df, monthyear):\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = 'bigdata_project_hksv'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    time1 = time.strftime(\"%Y%m%d-%H.%M.%S\")\n",
    "    name = 'tweetData/'+str(monthyear)+'_'+time1+'.csv'\n",
    "    blob = bucket.blob(name)\n",
    "    blob.upload_from_string(out_df.to_csv(), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_api(monthyear, tweet_id_list, out_df):\n",
    "    try:\n",
    "        out_df.set_index('tweet_id', inplace = True)\n",
    "\n",
    "        tweet_list = client.get_tweets(tweet_id_list)\n",
    "        ids = [str(i.id) for i in tweet_list.data]\n",
    "        text = [i.text for i in tweet_list.data]\n",
    "\n",
    "        out_df.loc[ids, 'tweet'] = text\n",
    "    \n",
    "        return out_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if '429' in str(e):\n",
    "            print(429)\n",
    "        pass\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df(df2):\n",
    "    if os.path.exists('logs.json'):\n",
    "        with open('logs.json') as json_file:\n",
    "            dict1 = json.load(json_file)\n",
    "        df2 = df2[df2.tweet_id.isin((set(dict1['ids']) - set(df2.tweet_id.to_list())))]\n",
    "        return df2\n",
    "    else:\n",
    "        return df2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(chunk):\n",
    "    df1 = chunk[chunk.columns[0]].str.split(expand = True)\n",
    "    df1.rename(columns={0: 'tweet_id', 1: 'tdate', 2: 'ttime', 3: 'tlang', 4: 'tcountry_place'}, inplace = True)\n",
    "    df2 = df1[df1['tlang'] == 'en']\n",
    "    \n",
    "    df2 = check_df(df2)\n",
    "    \n",
    "    #grouping dfs by month  and year\n",
    "    df2.tdate = pd.to_datetime(df2.tdate)\n",
    "    df2['month_year'] = df2['tdate'].dt.to_period('M')\n",
    "    df3 = df2.groupby(['month_year'])\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_json(appended_data):\n",
    "    appended_data.reset_index(inplace = True)\n",
    "    if os.path.exists('logs.json'):\n",
    "        with open('logs.json') as json_file:\n",
    "            dict1 = json.load(json_file)\n",
    "        for i in appended_data['tweet_id'].to_list():\n",
    "            dict1['ids'].append(i)\n",
    "        \n",
    "        f = open('logs.json', \"w\")\n",
    "        json.dump(dict1, f)\n",
    "        f.close()\n",
    "    \n",
    "    else:\n",
    "        dict1 = {}\n",
    "        dict1['ids'] = appended_data['tweet_id'].to_list()\n",
    "        f = open('logs.json', \"w\")\n",
    "        json.dump(dict1, f)\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10013b172bc430fbfb539b6e51a9315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunksize = 1000000\n",
    "\n",
    "tweets = []\n",
    "\n",
    "\n",
    "for chunk in tqdm(pd.read_csv('../data/full_dataset_clean.tsv', \n",
    "                         chunksize=chunksize, \n",
    "                         iterator=True)):\n",
    "\n",
    "\n",
    "    df3 = preprocessing(chunk)\n",
    "    \n",
    "    for monthyear in df3.groups.keys():\n",
    "        print(monthyear)\n",
    "        df4 = df3.get_group(monthyear)\n",
    "        tweet_id_list = list(map(int, df4['tweet_id'].tolist()))\n",
    "        df4.reset_index(inplace  = True, drop = True)\n",
    "        appended_data = []\n",
    "        for i in range(0, df4.shape[0], 100):\n",
    "            tweet_ids = tweet_id_list[i: i+100]\n",
    "            out_df = twitter_api(monthyear,tweet_ids,df4.iloc[i:i+100])\n",
    "            appended_data.append(out_df)\n",
    "        appended_data = pd.concat(appended_data)\n",
    "        upload_to_gcp(appended_data, monthyear)\n",
    "        write_to_json(appended_data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation with DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "import numpy as np\n",
    "# start = time.time()\n",
    "dask_df = dd.read_csv('../data/full_dataset_clean.tsv')\n",
    "# end = time.time()\n",
    "# print(\"Read csv with dask: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dask_df[dask_df.columns[0]].str.split(n=4, expand = True)\n",
    "\n",
    "\n",
    "df3 = df2.rename(columns={0: 'tweet_id', 1: 'tdate', 2: 'ttime', 3: 'tlang', 4: 'tcountry_place'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[df3['tlang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(id):\n",
    "    try:\n",
    "#         print(id)\n",
    "        return api.get_status(id).text\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['tweet'] = df4['tweet_id'].apply(lambda x: get_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
