{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "import json\n",
    "import time\n",
    "from google.cloud import storage"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "file_dict = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Read csv files from separate folders and create a dictionary file_dict of form {df_name: df}\n",
    "\"\"\"\n",
    "def read_csv(date,datelong):   \n",
    "    path = '/Users/shrutigandhi/Desktop/MScA UChicago/Quarter 1/31013 Big Data/Project/CoAID-master/'+datelong+'/'\n",
    "    os.chdir(path)\n",
    "    extension = \"csv\"\n",
    "    files = glob.glob('*.{}'.format(extension))\n",
    "    for file in files:\n",
    "        tag = \"Real\" if file.find(\"Real\") > 0 else \"Fake\"\n",
    "        source = \"Claim\" if file.find(\"News\") < 0 else \"News\"\n",
    "        tweet_reply = True if file.find(\"tweets_replies\") > 0 else False\n",
    "        tweet = True if file.find(\"tweets.\") > 0 else False\n",
    "    \n",
    "        if tweet == True:\n",
    "            key = source+\"_\"+tag+\"_tweets_\"+date\n",
    "            df = pd.read_csv(source+tag+\"COVID-19_tweets.csv\")\n",
    "        elif tweet_reply == True:\n",
    "            key = source+\"_\"+tag+\"_tweets_replies_\"+date\n",
    "            df = pd.read_csv(source+tag+\"COVID-19_tweets_replies.csv\")\n",
    "        else:\n",
    "            key = source+\"_\"+tag+\"_\"+date\n",
    "            df = pd.read_csv(source+tag+\"COVID-19.csv\")\n",
    "        file_dict[key] = df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "read_csv(\"jan5\",\"05-01-2020\")\n",
    "read_csv(\"jan7\",\"07-01-2020\")\n",
    "read_csv(\"jan9\",\"09-01-2020\")\n",
    "read_csv(\"jan11\",\"11-01-2020\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checks\n",
    "len(file_dict.keys())\n",
    "file_dict.keys()\n",
    "file_dict['News_Real_tweets_replies_jan5'].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create tags, source, and date columns in the dfs\n",
    "dates = [\"jan5\",\"jan7\",\"jan9\",\"jan11\"]\n",
    "tags = [\"Fake\",\"Real\"]\n",
    "sources = [\"News\",\"Claim\"]\n",
    "\n",
    "for key in file_dict.keys():\n",
    "    for i in dates:\n",
    "        if i in key:\n",
    "            file_dict[key][\"date\"] = i\n",
    "\n",
    "for key in file_dict.keys():\n",
    "    for i in tags:\n",
    "        if i in key:\n",
    "            file_dict[key][\"tag\"] = i\n",
    "\n",
    "for key in file_dict.keys():\n",
    "    for i in sources:\n",
    "        if i in key:\n",
    "            file_dict[key][\"source\"] = i"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# append entire data\n",
    "appended_data = pd.DataFrame()\n",
    "for key in file_dict.keys():\n",
    "    appended_data = appended_data.append([file_dict[key]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checks\n",
    "appended_data.shape\n",
    "appended_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extracting Tweets and Uploading to GCS Bucket"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"even-hull-328204-ce42cf63b52f.json\"\n",
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAFcOVAEAAAAA448MSlftb%2Bjape1DZl0OO2LFppA%3Dp8iQlwSFlyDTdEZuyaxuGXbbgzgtblgFM63N5MIZERKx59BjBP',\n",
    "                      wait_on_rate_limit=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CONSUMER_KEY = 'RAZ2FeXVy0xA91xx2HbzNxDwv'\n",
    "CONSUMER_SECRET = 'kd4kQWPbzms3dzLj1g2ngSfzsNLTJAHf3nnhvLTzSikiNA7Jjg'\n",
    "OAUTH_TOKEN= '1672431301-dNVTlYEE6OlltS53tCTAGFfF47Y7j74WPcommDs'\n",
    "OAUTH_TOKEN_SECRET = 'YBghh7QRPMRLI36xrct9WEcROvqLZqG9UCJrDkC268ala'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "upload the final dataset to gcs in the specified folder\n",
    "\"\"\"\n",
    "def upload_to_gcp(out_df):\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = 'bigdata_project_hksv'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    name = 'CoAID_cleaned_data/coaid_clean_data.csv'\n",
    "    blob = bucket.blob(name)\n",
    "    blob.upload_from_string(out_df.to_csv(), 'text/csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "extract tweets given list of tweet_ids, and create a field in out_df to put in the tweet text\n",
    "\"\"\"\n",
    "def twitter_api(tweet_id_list, out_df, field, index):\n",
    "    out_df.set_index(index, inplace = True)\n",
    "\n",
    "    tweet_list = client.get_tweets(tweet_id_list)\n",
    "    ids = [float(i.id) for i in tweet_list.data]\n",
    "    text = [i.text for i in tweet_list.data]\n",
    "\n",
    "    out_df.loc[ids, field] = text\n",
    "\n",
    "    return out_df\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         if '429' in str(e):\n",
    "#             print(429)\n",
    "#         pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_df(df2):\n",
    "    if os.path.exists('logs.json'):\n",
    "        with open('logs.json') as json_file:\n",
    "            dict1 = json.load(json_file)\n",
    "        df2 = df2[df2.tweet_id.isin((set(df2.tweet_id.to_list()) - set(dict1['ids'])))]\n",
    "        return df2\n",
    "    else:\n",
    "        return df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "drop duplicate and missing tweet_ids/reply_ids\n",
    "\"\"\"\n",
    "def preprocessing(data, field):\n",
    "    df1 = data[data[field].notna()]\n",
    "    df1 = df1.drop_duplicates(subset=[field])\n",
    "    df2 = check_df(df1)\n",
    "    return df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def write_to_json(appended_data):\n",
    "    appended_data.reset_index(inplace = True)\n",
    "    if os.path.exists('logs.json'):\n",
    "        with open('logs.json') as json_file:\n",
    "            dict1 = json.load(json_file)\n",
    "        for i in appended_data['tweet_id'].to_list():\n",
    "            dict1['ids'].append(i)\n",
    "        \n",
    "        f = open('logs.json', \"w\")\n",
    "        json.dump(dict1, f)\n",
    "        f.close()\n",
    "    \n",
    "    else:\n",
    "        dict1 = {}\n",
    "        dict1['ids'] = appended_data['tweet_id'].to_list()\n",
    "        f = open('logs.json', \"w\")\n",
    "        json.dump(dict1, f)\n",
    "        f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets = []\n",
    "df3 = preprocessing(appended_data, \"tweet_id\")\n",
    "df3_reply = preprocessing(appended_data, \"reply_id\")\n",
    "\n",
    "\n",
    "tweet_id_list = list(map(int, df3['tweet_id'].tolist()))\n",
    "df3.reset_index(inplace  = True, drop = True)\n",
    "appended_list_new = []\n",
    "for i in range(0, df3.shape[0], 100):\n",
    "    tweet_ids = tweet_id_list[i: i+100]\n",
    "    out_df = twitter_api(tweet_ids,df3.iloc[i:i+100],'tweet','tweet_id')\n",
    "    appended_list_new.append(out_df)\n",
    "appended_list_new = pd.concat(appended_list_new) # data with all tweetids and text (unique and not null)\n",
    "write_to_json(appended_list_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweet_id_list_reply = list(map(int, df3_reply['reply_id'].tolist()))\n",
    "df3_reply.reset_index(inplace  = True, drop = True)\n",
    "appended_list_new_reply = []\n",
    "for i in range(0, df3_reply.shape[0], 100):\n",
    "    tweet_ids_reply = tweet_id_list_reply[i: i+100]\n",
    "    out_df_reply = twitter_api(tweet_ids_reply,df3_reply.iloc[i:i+100],'reply','reply_id')\n",
    "    appended_list_new_reply.append(out_df_reply)\n",
    "appended_list_new_reply = pd.concat(appended_list_new_reply) # data with all replyids and text (unique and not null)\n",
    "write_to_json(appended_list_new_reply)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "req_cols = appended_list_new[['tweet_id', 'tweet']] # keeping only tweet_id and tweet\n",
    "appended_merge_1 = appended_data.merge(req_cols, on='tweet_id', how='left') # left_join with base data containing duplicate tweet_ids on left"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "req_cols_reply = appended_list_new_reply[['reply_id', 'reply']] # keeping only reply_id and tweet replies\n",
    "coaid_clean_data = appended_merge_1.merge(req_cols_reply, on='reply_id', how='left') # left_join with merged data created in last step on left"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(coaid_clean_data[coaid_clean_data[\"tweet\"].notna()].shape,\n",
    "      coaid_clean_data[coaid_clean_data[\"reply\"].notna()].shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "upload_to_gcp(coaid_clean_data)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ebdfeb8887924e606afbfc51e32509445d9115704a3d5d2909aacca2fb2beb12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}